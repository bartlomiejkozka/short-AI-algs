{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe0742a",
   "metadata": {},
   "source": [
    "# Approximation a nonlinear function using a feedforward neural network, trained on noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750fb23",
   "metadata": {},
   "source": [
    "## Approximated function\n",
    "#### y = 0.5*cos(0.2.*x.^2)+0.5;\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4342e7",
   "metadata": {},
   "source": [
    "## Through algorithm process\n",
    "\n",
    "To the target function a random noise is added to simulate real-world imperfections. \n",
    "A neural network firstly with one and then with two hidden layers is trained to learn the pattern, and its predictions are compared to the original function to evaluate the quality of approximation.\n",
    "\n",
    "![](temp.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f26da",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701c380",
   "metadata": {},
   "source": [
    "### Fixed training set and dynamic number of neurons and hidden layers (1/2 layers)\n",
    "- number of training data = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6131b",
   "metadata": {},
   "source": [
    "#### a) Test with the nurons count equals appropirately to proposed formula that count of nerons in hidden layer equals to square root of m multiplied by n, where m is count of input data (nuerons) and n is count of output data (nurons): k = sqrt( m * n )\n",
    "\n",
    "- hidden layer = 1\n",
    "- nuerons count in hidden layer = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9d76f",
   "metadata": {},
   "source": [
    "| Training | Validation |\n",
    "|----------|------------|\n",
    "| ![Training](n500l1n1.jpg) | ![Validation](n500l1n1-regression.jpg) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9a545",
   "metadata": {},
   "source": [
    "- mean square error: ~ 0.1\n",
    "Summarizing, the above propozed formula about number of nurons in hidden layer is not good for not linear funcitons, line in our example.\n",
    "\n",
    "The mean square error ispreety big in scale of our values thaht function can have, and also the regression plots show that model predict bad in every sets, so is not event good in training set of test set. \\\n",
    "For values which starts from 0.5 and up, the model prediction is totaly bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ff438",
   "metadata": {},
   "source": [
    "#### b) Test with various number of nurons in hidden layer\n",
    "\n",
    "- hidden layer = 1\n",
    "- nuerons count in hidden layer = from 2 to 30 by step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3950a0c",
   "metadata": {},
   "source": [
    "![1 hidden layer, 2-30 neurons](n500l1n2-30.jpg)\n",
    "![1 hidden layer, 2-30 neurons - MSE](n500l1n2-30-mse.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa579511",
   "metadata": {},
   "source": [
    "In range of small values for neurons count in hidden layer, the model is to general to our data points, so we can observer model underfitting. \\\n",
    "In opposite situation, for range with bigger values of neurons count, the model is too accurate for every single points od data, which makes its very sensitive for every noise is our data, and this is caled model overfitting.\n",
    "\n",
    "Summarizing, abve plots figure of model predictions and the plot of MSE error clearly inlustrate the good range of neurons in hidden layer, to make model prediction appropraite to real values.\n",
    "\n",
    "Count of min and max number od neurons in hidden layer to make good prediction:\n",
    "- min = ~ 10\n",
    "- max = ~ 14\n",
    "\n",
    "For this range of count of neurons in hidden layer, the MSE is pretty small ~ 0.001, so we can assume that this range in our case for single layer is appropraite to make model predictions satisfactory in compare with real values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c2470",
   "metadata": {},
   "source": [
    "##### Pedictions for best neurons count in hidden layer\n",
    "\n",
    "We can exactly see for what neurons number the model perform the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defc1bb",
   "metadata": {},
   "source": [
    "- 10 neurons\n",
    "- MSE = ~ 0.009\n",
    "\n",
    "| Training | Validation |\n",
    "|----------|------------|\n",
    "| ![10 neurons](n500l1n10.jpg) | ![10 neurons](n500l1n10-regression.jpg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e608d",
   "metadata": {},
   "source": [
    "- 12 neurons\n",
    "- MSE = ~ 0.0002\n",
    "\n",
    "| Training | Validation |\n",
    "|----------|------------|\n",
    "| ![10 neurons](n500l1n12.jpg) | ![10 neurons](n500l1n12-regression.jpg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc4c9a",
   "metadata": {},
   "source": [
    "- 14 neurons\n",
    "- MSE = ~ 0.0005\n",
    "\n",
    "| Training | Validation |\n",
    "|----------|------------|\n",
    "| ![10 neurons](n500l1n14.jpg) | ![10 neurons](n500l1n14-regression.jpg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffaf394",
   "metadata": {},
   "source": [
    "Summarizing, from above possiblities of neruons (10,12,14) in single hiddne layer **the best one seems to be with 12 neurons**\n",
    "\n",
    "We can doubt this choice, because the best Pearson’s linear correlation coefficient (the highest) is with the 14 neurons in layer, but there the MSE error is much bigger than with 12 nurons. \\\n",
    "\n",
    "We prioritize the MSE Error value than R value, because the R value can be high and also the predict values can be far from the real ones, e.g. when the MSE error will be constant big, but also the tendancy about rising predicted values and rising in te same time real values will be maintained.\n",
    "\n",
    "So with single hidden layer the best compromise between small and big amount of nuerons is the 12 neurons in hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe50d38",
   "metadata": {},
   "source": [
    "#### Can second layer addition improve the models predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540165a0",
   "metadata": {},
   "source": [
    "- hidden layer = 2\n",
    "- nuerons count in hidden layer = from 2 to 30 by step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd428e",
   "metadata": {},
   "source": [
    "![](n500l2n2-30.jpg)\n",
    "![](n500l2n2-30-mse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1c5b8",
   "metadata": {},
   "source": [
    "Summarizing, from above plots, the best seems to be ~ 5 and ~ 12 nuerons in hidden 2 layers, so lets check them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80783c44",
   "metadata": {},
   "source": [
    "- 5 nerons, 2 layers : MSE = ~ 0.0006\n",
    "\n",
    "- 12 neurons, 2 layers : MSE = ~ 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0ed73",
   "metadata": {},
   "source": [
    "- 14 neurons\n",
    "- MSE = ~ 0.0005\n",
    "\n",
    "| Training | Training   |\n",
    "|----------|------------|\n",
    "| ![10 neurons](n500l2n5.jpg) | ![10 neurons](n500l2n12.jpg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50beac3c",
   "metadata": {},
   "source": [
    "Summarizing, the second layer addition is not valuable for our no linear funciton value prediction, the best is 5 nerons for 2 layers, bit still it has worse results in MSE, so the MSE is bigger than in single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcff6b",
   "metadata": {},
   "source": [
    "### For the optimized number of neurons, lets manipulate the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71106b7",
   "metadata": {},
   "source": [
    "![](n100-5000.jpg)\n",
    "![](n100-5000-mse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b5c9",
   "metadata": {},
   "source": [
    "So as we can see from above plots, the general tendence is that when the more points (input data) is, then the model predict better and better. \\\n",
    "\n",
    "We can also see that we tendency line from about 1000 input data is const, especially from about 3500 counts of input data the model prediction is on the same good level. \\\n",
    "\n",
    "So in comparisson, lets choose 4000 input data and analyse this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b5a3a",
   "metadata": {},
   "source": [
    "**Consequances of low number of data inputs**\n",
    "\n",
    "These effects are also clearly visible in the case with a smaller number of samples ~ 100.\n",
    "The model tends to overfit the noisy data, shows poor generalization, and its predictions are less stable and less accurate due to the limited representativeness of the training set. So in case of low count of data inputs, there is model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d9030",
   "metadata": {},
   "source": [
    "- input data = 4000\n",
    "- layers = 1\n",
    "- nerons in hidden layer = 12\n",
    "\n",
    "\n",
    "![](comp500-4000.jpg)\n",
    "\n",
    "Model 1 (500 próbek):\n",
    "  MSE  = 0.00142\n",
    "  MAE  = 0.02666\n",
    "  RMSE = 0.03774\n",
    "  R²   = 0.98934\n",
    "\n",
    "Model 2 (4000 próbek):\n",
    "  MSE  = 0.00020\n",
    "  MAE  = 0.01020\n",
    "  RMSE = 0.01403\n",
    "  R²   = 0.99853\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59ae44",
   "metadata": {},
   "source": [
    "Above example and metrics confirm our earlier prediction that with increasing input data, the model is able to make better predictions.\n",
    "This improvement is especially visible in metrics like R² and MAE, which reflect the model's ability to generalize well to the true underlying function, not just noisy training points.\n",
    "More input data helps the model avoid overfitting by providing a more diverse and representative sample of the target function. As a result, the model learns the actual patterns rather than memorizing noise or outliers.\n",
    "However, it’s also important to balance data size with model complexity — in very large datasets, overly complex models may still overfit noise if regularization or early stopping is not applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d28f7",
   "metadata": {},
   "source": [
    "#### Evaluation of the impact of training duration on the effect of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a9702",
   "metadata": {},
   "source": [
    "| Limited Training (up to 6 epoch since no improvment) | Not limited Training   |\n",
    "|----------|------------|\n",
    "| ![10 neurons](limited.jpg) | ![10 neurons](not-limited.jpg)|\n",
    "| ![10 neurons](limited-mse.jpg) | ![10 neurons](not-limited-mse.jpg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f2dcc",
   "metadata": {},
   "source": [
    "The two approaches illustrate how early stopping (limiting training when no improvement is observed) helps in preventing overfitting.\n",
    "\n",
    "- Limited Training (Early Stopping): \\\n",
    "In this case, the training process stops automatically after 6 epochs without validation improvement. This helps the model generalize better, as seen in the smoother approximation curve and lower validation error (best at epoch 27, MSE ≈ 0.00847). The model does not try to fit the noise in the data, maintaining a good balance between bias and variance.\n",
    "- Unrestricted Training: \\\n",
    "Training continues for all 100 epochs, regardless of validation performance. As shown in the graph, the model starts to overfit: while training error continues to decrease, validation and test errors start to increase after a point (best validation performance was actually earlier, at epoch 24 with MSE ≈ 0.01026). The approximation curve shows more unnecessary oscillations, indicating that the model has begun fitting the noise in the training data.\n",
    "\n",
    "\n",
    "Conclusion: \\\n",
    "Early stopping is an effective regularization technique to reduce overfitting. It halts training when further improvements on the validation set cease, ensuring the model retains generalization capability without fitting irrelevant patterns or noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
